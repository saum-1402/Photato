{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":33884,"sourceType":"datasetVersion","datasetId":1864}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader, Subset, random_split\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score\nimport numpy as np\nfrom tqdm import tqdm\nimport csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T14:53:17.408414Z","iopub.execute_input":"2024-12-09T14:53:17.408743Z","iopub.status.idle":"2024-12-09T14:53:17.413653Z","shell.execute_reply.started":"2024-12-09T14:53:17.408719Z","shell.execute_reply":"2024-12-09T14:53:17.412837Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Loading and Splitting the data","metadata":{}},{"cell_type":"markdown","source":"Normalizing also","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Dataset and transforms\ndata_dir = \"/kaggle/input/food41/images\"\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),  # Resize to EfficientNet-B7's expected size\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ndataset = datasets.ImageFolder(root=data_dir, transform=transform)\n\n# Split dataset into train, validation, and test\nindices = list(range(len(dataset)))\ntrain_indices, test_indices = train_test_split(\n    indices, test_size=0.2, random_state=42, stratify=dataset.targets\n)\ntrain_dataset = Subset(dataset, train_indices)\ntest_dataset = Subset(dataset, test_indices)\n\nval_ratio = 0.5  # 50% of the test set will be used for validation\nval_size = int(len(test_dataset) * val_ratio)\ntest_size = len(test_dataset) - val_size\nval_dataset, test_dataset = random_split(test_dataset, [val_size, test_size])\n\n# Data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=8)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=8)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=8)\n\nprint(f\"Training images: {len(train_dataset)}\")\nprint(f\"Validation images: {len(val_dataset)}\")\nprint(f\"Testing images: {len(test_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T14:53:17.415519Z","iopub.execute_input":"2024-12-09T14:53:17.416088Z","iopub.status.idle":"2024-12-09T14:55:02.615487Z","shell.execute_reply.started":"2024-12-09T14:53:17.416049Z","shell.execute_reply":"2024-12-09T14:55:02.614598Z"}},"outputs":[{"name":"stdout","text":"Training images: 80800\nValidation images: 10100\nTesting images: 10100\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"model = models.efficientnet_b4(pretrained=True)\nnum_classes = len(dataset.classes)  # Number of classes in the dataset\nmodel.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)  # Modify classifier\nmodel = model.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.00005)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T14:55:02.616530Z","iopub.execute_input":"2024-12-09T14:55:02.616817Z","iopub.status.idle":"2024-12-09T14:55:03.799748Z","shell.execute_reply.started":"2024-12-09T14:55:02.616792Z","shell.execute_reply":"2024-12-09T14:55:03.799070Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B4_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B4_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/efficientnet_b4_rwightman-23ab8bcd.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b4_rwightman-23ab8bcd.pth\n100%|██████████| 74.5M/74.5M [00:00<00:00, 198MB/s]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"csv_file = \"training_metrics.csv\"\nwith open(csv_file, mode=\"w\", newline=\"\") as file:\n    writer = csv.writer(file)\n    writer.writerow([\"Epoch\", \"Train Loss\", \"Train Accuracy\", \"Val Loss\", \"Val Accuracy\"])\n\nepochs = 1\nbest_val_loss = float(\"inf\")\nbest_model_path = \"best_model.pth\"\n\nfor epoch in range(epochs):\n    model.train()\n    train_loss, train_correct = 0.0, 0\n\n    # Training phase\n    with tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\") as pbar:\n        for inputs, labels in pbar:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n            _, preds = torch.max(outputs, 1)\n            train_correct += (preds == labels).sum().item()\n\n            pbar.set_postfix(loss=train_loss / len(train_loader))\n\n    train_loss /= len(train_loader)\n    train_accuracy = train_correct / len(train_dataset)\n\n    # Validation phase\n    model.eval()\n    val_loss, val_correct = 0.0, 0\n\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n\n            _, preds = torch.max(outputs, 1)\n            val_correct += (preds == labels).sum().item()\n\n    val_loss /= len(val_loader)\n    val_accuracy = val_correct / len(val_dataset)\n\n    with open(csv_file, mode=\"a\", newline=\"\") as file:\n        writer = csv.writer(file)\n        writer.writerow([epoch + 1, train_loss, train_accuracy, val_loss, val_accuracy])\n\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save({\n            \"model\": model,\n            \"classes\": dataset.classes\n        }, best_model_path)\n        print(f\"New best model saved with validation loss: {best_val_loss:.4f}\")\n\n    print(f\"Epoch {epoch + 1}/{epochs} -> Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T14:55:03.800851Z","iopub.execute_input":"2024-12-09T14:55:03.801124Z","iopub.status.idle":"2024-12-09T15:08:37.621252Z","shell.execute_reply.started":"2024-12-09T14:55:03.801099Z","shell.execute_reply":"2024-12-09T15:08:37.620157Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/1: 100%|██████████| 2525/2525 [13:05<00:00,  3.22it/s, loss=2.81]  \n","output_type":"stream"},{"name":"stdout","text":"New best model saved with validation loss: 1.4795\nEpoch 1/1 -> Train Loss: 2.8082, Train Acc: 0.3464, Val Loss: 1.4795, Val Acc: 0.6206\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Loading the model and evaluating on the test set","metadata":{}},{"cell_type":"code","source":"checkpoint = torch.load(best_model_path, map_location=device)\nmodel = checkpoint[\"model\"]\nclass_names = checkpoint[\"classes\"]\nmodel = model.to(device)\nmodel.eval()\n\n\ndef evaluate_model(loader):\n    all_preds, all_labels = [], []\n\n    with torch.no_grad():\n        for inputs, labels in loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    return np.array(all_preds), np.array(all_labels)\n\ntest_preds, test_labels = evaluate_model(test_loader)\n\nprint(\"Test Classification Report:\")\nprint(classification_report(test_labels, test_preds, target_names=class_names))\ntest_accuracy = accuracy_score(test_labels, test_preds)\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T15:08:37.623411Z","iopub.execute_input":"2024-12-09T15:08:37.623731Z","iopub.status.idle":"2024-12-09T15:09:05.782514Z","shell.execute_reply.started":"2024-12-09T15:08:37.623700Z","shell.execute_reply":"2024-12-09T15:09:05.781461Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_23/447767184.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(best_model_path, map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"Test Classification Report:\n                         precision    recall  f1-score   support\n\n              apple_pie       0.40      0.29      0.34       100\n         baby_back_ribs       0.55      0.72      0.62        92\n                baklava       0.68      0.74      0.71       108\n         beef_carpaccio       0.62      0.62      0.62       104\n           beef_tartare       0.62      0.53      0.57        94\n             beet_salad       0.46      0.44      0.45        98\n               beignets       0.81      0.78      0.79       107\n               bibimbap       0.68      0.85      0.75       101\n          bread_pudding       0.47      0.19      0.27        90\n      breakfast_burrito       0.60      0.61      0.61        96\n             bruschetta       0.55      0.48      0.51       107\n           caesar_salad       0.70      0.69      0.70        98\n                cannoli       0.66      0.72      0.69       103\n          caprese_salad       0.48      0.65      0.55        92\n            carrot_cake       0.76      0.48      0.59        98\n                ceviche       0.42      0.42      0.42       106\n           cheese_plate       0.49      0.53      0.51        93\n             cheesecake       0.56      0.46      0.50        90\n          chicken_curry       0.67      0.45      0.54       106\n     chicken_quesadilla       0.58      0.65      0.61       109\n          chicken_wings       0.63      0.76      0.69       100\n         chocolate_cake       0.55      0.60      0.58        93\n       chocolate_mousse       0.30      0.34      0.32        89\n                churros       0.76      0.83      0.79        94\n           clam_chowder       0.75      0.77      0.76        98\n          club_sandwich       0.67      0.59      0.63       105\n             crab_cakes       0.47      0.44      0.46       105\n           creme_brulee       0.78      0.73      0.76        94\n          croque_madame       0.68      0.65      0.67        80\n              cup_cakes       0.62      0.81      0.70       103\n           deviled_eggs       0.73      0.67      0.70        94\n                 donuts       0.65      0.60      0.62       104\n              dumplings       0.86      0.70      0.78       108\n                edamame       0.98      0.95      0.96        98\n          eggs_benedict       0.58      0.75      0.65        99\n              escargots       0.78      0.63      0.69       105\n                falafel       0.53      0.57      0.55        97\n           filet_mignon       0.50      0.41      0.45       103\n         fish_and_chips       0.74      0.73      0.74       100\n              foie_gras       0.52      0.26      0.34       109\n           french_fries       0.74      0.84      0.79       100\n      french_onion_soup       0.56      0.73      0.63        99\n           french_toast       0.50      0.46      0.48        90\n         fried_calamari       0.73      0.68      0.71        98\n             fried_rice       0.69      0.74      0.71        95\n          frozen_yogurt       0.70      0.88      0.78       104\n           garlic_bread       0.64      0.52      0.57        94\n                gnocchi       0.68      0.53      0.59        95\n            greek_salad       0.61      0.65      0.63       100\ngrilled_cheese_sandwich       0.43      0.49      0.46        91\n         grilled_salmon       0.52      0.38      0.44       100\n              guacamole       0.68      0.86      0.76        88\n                  gyoza       0.80      0.67      0.73       106\n              hamburger       0.55      0.76      0.64        93\n      hot_and_sour_soup       0.89      0.89      0.89        96\n                hot_dog       0.68      0.78      0.73        95\n       huevos_rancheros       0.56      0.35      0.43       104\n                 hummus       0.69      0.23      0.35       117\n              ice_cream       0.48      0.62      0.54        95\n                lasagna       0.59      0.62      0.61       107\n         lobster_bisque       0.77      0.77      0.77        95\n  lobster_roll_sandwich       0.64      0.68      0.66        98\n    macaroni_and_cheese       0.72      0.63      0.67       108\n               macarons       0.66      0.94      0.78        99\n              miso_soup       0.80      0.90      0.85        96\n                mussels       0.76      0.83      0.80        94\n                 nachos       0.60      0.43      0.50       106\n               omelette       0.40      0.50      0.45       103\n            onion_rings       0.77      0.88      0.82       103\n                oysters       0.80      0.92      0.85       102\n               pad_thai       0.64      0.85      0.73       101\n                 paella       0.53      0.70      0.60       107\n               pancakes       0.64      0.71      0.67        96\n            panna_cotta       0.37      0.58      0.45        91\n            peking_duck       0.67      0.48      0.56       102\n                    pho       0.73      0.95      0.82        98\n                  pizza       0.62      0.85      0.72        94\n              pork_chop       0.38      0.38      0.38       105\n                poutine       0.74      0.65      0.69        99\n              prime_rib       0.56      0.79      0.66       105\n   pulled_pork_sandwich       0.72      0.40      0.52        94\n                  ramen       0.84      0.51      0.63        97\n                ravioli       0.53      0.25      0.34       105\n        red_velvet_cake       0.57      0.81      0.67        83\n                risotto       0.50      0.62      0.55       109\n                 samosa       0.63      0.67      0.65       108\n                sashimi       0.73      0.80      0.76       106\n               scallops       0.43      0.48      0.45       108\n          seaweed_salad       0.83      0.91      0.87        98\n       shrimp_and_grits       0.42      0.61      0.50       106\n    spaghetti_bolognese       0.84      0.74      0.78       106\n    spaghetti_carbonara       0.73      0.86      0.79        98\n           spring_rolls       0.76      0.68      0.72       119\n                  steak       0.45      0.19      0.27       101\n   strawberry_shortcake       0.66      0.58      0.62       113\n                  sushi       0.61      0.61      0.61       106\n                  tacos       0.41      0.42      0.42        97\n               takoyaki       0.66      0.61      0.63       107\n               tiramisu       0.52      0.45      0.48        98\n           tuna_tartare       0.45      0.33      0.38        92\n                waffles       0.80      0.60      0.68       110\n\n               accuracy                           0.63     10100\n              macro avg       0.63      0.63      0.62     10100\n           weighted avg       0.63      0.63      0.62     10100\n\nTest Accuracy: 0.6253\n","output_type":"stream"}],"execution_count":7}]}